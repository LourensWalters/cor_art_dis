{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Baseline Models and Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This notebook adapts the feature engineering from the original paper to our windowed approach.  We use linear & other\n",
    " simple regression models here to serve as a baseline for the deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import ElasticNet, LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "# change working directory to base, to make all imports and file paths work\n",
    "import os\n",
    "#os.chdir(os.pardir)\n",
    "os.chdir(r\"C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\")\n",
    "print(\"Current directory: %s\" % os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/processed/rebuild_windowed_features_50_5_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array(['cell_key', 'cell_batch', 'cell_num', 'minimum_dQ_window',\n       'variance_dQ_window', 'skewness_dQ_window', 'kurtosis_dQ_window',\n       'slope_lin_fit_window', 'intercept_lin_fit_window',\n       'discharge_capacity_1', 'diff_discharge_capacity_max_1',\n       'mean_discharge_time', 'minimum_IR_window', 'diff_IR_window',\n       'target_remaining', 'target_current', 'target_classifier'],\n      dtype=object)"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    4148\n",
      "1.0    2326\n",
      "Name: target_classifier, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"target_classifier\"].value_counts())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "       cell_batch     cell_num  minimum_dQ_window  variance_dQ_window  \\\ncount      6474.0  6474.000000        6474.000000         6474.000000   \nmean          1.0    21.669601          -4.033639           -9.876923   \nstd           0.0    13.910576           1.233619            1.361310   \nmin           1.0     0.000000         -15.232677          -13.121696   \n25%           1.0     7.000000          -4.395173          -10.836907   \n50%           1.0    23.000000          -3.796769           -9.986102   \n75%           1.0    33.000000          -3.289231           -8.909548   \nmax           1.0    45.000000          -0.639437           -5.993006   \n\n       skewness_dQ_window  kurtosis_dQ_window  slope_lin_fit_window  \\\ncount         6474.000000         6474.000000           6474.000000   \nmean            -0.426947           -0.406053             -0.000205   \nstd              1.010082            1.128265              0.000287   \nmin             -8.627348           -7.862580             -0.003821   \n25%             -0.853847           -0.995698             -0.000229   \n50%             -0.154603           -0.465569             -0.000077   \n75%              0.184834            0.164710             -0.000043   \nmax              1.246655            3.428991              0.002369   \n\n       intercept_lin_fit_window  discharge_capacity_1  \\\ncount               6474.000000           6474.000000   \nmean                   1.053025              1.052759   \nstd                    0.034846              0.035436   \nmin                    0.922702              0.922034   \n25%                    1.042337              1.042055   \n50%                    1.064146              1.064172   \n75%                    1.076654              1.076539   \nmax                    1.197957              1.539054   \n\n       diff_discharge_capacity_max_1  mean_discharge_time  minimum_IR_window  \\\ncount                    6474.000000          6474.000000        6474.000000   \nmean                        0.002700            13.735394           0.016433   \nstd                         0.064285             0.778931           0.002014   \nmin                         0.000000            11.059120           0.000000   \n25%                         0.000000            13.487345           0.016255   \n50%                         0.000000            13.974640           0.016636   \n75%                         0.000205            14.282609           0.016990   \nmax                         1.817914            14.772992           0.019886   \n\n       diff_IR_window  target_remaining  target_current  target_classifier  \ncount     6474.000000       6474.000000      6474.00000        6474.000000  \nmean         0.000147        419.687519       465.78962           0.359283  \nstd          0.000592        267.329307       267.28278           0.479827  \nmin         -0.004035          2.000000        50.00000           0.000000  \n25%         -0.000025        199.000000       245.00000           0.000000  \n50%          0.000082        396.000000       441.00000           0.000000  \n75%          0.000222        602.000000       650.00000           1.000000  \nmax          0.017966       1176.000000      1221.00000           1.000000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cell_batch</th>\n      <th>cell_num</th>\n      <th>minimum_dQ_window</th>\n      <th>variance_dQ_window</th>\n      <th>skewness_dQ_window</th>\n      <th>kurtosis_dQ_window</th>\n      <th>slope_lin_fit_window</th>\n      <th>intercept_lin_fit_window</th>\n      <th>discharge_capacity_1</th>\n      <th>diff_discharge_capacity_max_1</th>\n      <th>mean_discharge_time</th>\n      <th>minimum_IR_window</th>\n      <th>diff_IR_window</th>\n      <th>target_remaining</th>\n      <th>target_current</th>\n      <th>target_classifier</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>6474.0</td>\n      <td>6474.000000</td>\n      <td>6474.000000</td>\n      <td>6474.000000</td>\n      <td>6474.000000</td>\n      <td>6474.000000</td>\n      <td>6474.000000</td>\n      <td>6474.000000</td>\n      <td>6474.000000</td>\n      <td>6474.000000</td>\n      <td>6474.000000</td>\n      <td>6474.000000</td>\n      <td>6474.000000</td>\n      <td>6474.000000</td>\n      <td>6474.00000</td>\n      <td>6474.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1.0</td>\n      <td>21.669601</td>\n      <td>-4.033639</td>\n      <td>-9.876923</td>\n      <td>-0.426947</td>\n      <td>-0.406053</td>\n      <td>-0.000205</td>\n      <td>1.053025</td>\n      <td>1.052759</td>\n      <td>0.002700</td>\n      <td>13.735394</td>\n      <td>0.016433</td>\n      <td>0.000147</td>\n      <td>419.687519</td>\n      <td>465.78962</td>\n      <td>0.359283</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.0</td>\n      <td>13.910576</td>\n      <td>1.233619</td>\n      <td>1.361310</td>\n      <td>1.010082</td>\n      <td>1.128265</td>\n      <td>0.000287</td>\n      <td>0.034846</td>\n      <td>0.035436</td>\n      <td>0.064285</td>\n      <td>0.778931</td>\n      <td>0.002014</td>\n      <td>0.000592</td>\n      <td>267.329307</td>\n      <td>267.28278</td>\n      <td>0.479827</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.0</td>\n      <td>0.000000</td>\n      <td>-15.232677</td>\n      <td>-13.121696</td>\n      <td>-8.627348</td>\n      <td>-7.862580</td>\n      <td>-0.003821</td>\n      <td>0.922702</td>\n      <td>0.922034</td>\n      <td>0.000000</td>\n      <td>11.059120</td>\n      <td>0.000000</td>\n      <td>-0.004035</td>\n      <td>2.000000</td>\n      <td>50.00000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1.0</td>\n      <td>7.000000</td>\n      <td>-4.395173</td>\n      <td>-10.836907</td>\n      <td>-0.853847</td>\n      <td>-0.995698</td>\n      <td>-0.000229</td>\n      <td>1.042337</td>\n      <td>1.042055</td>\n      <td>0.000000</td>\n      <td>13.487345</td>\n      <td>0.016255</td>\n      <td>-0.000025</td>\n      <td>199.000000</td>\n      <td>245.00000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1.0</td>\n      <td>23.000000</td>\n      <td>-3.796769</td>\n      <td>-9.986102</td>\n      <td>-0.154603</td>\n      <td>-0.465569</td>\n      <td>-0.000077</td>\n      <td>1.064146</td>\n      <td>1.064172</td>\n      <td>0.000000</td>\n      <td>13.974640</td>\n      <td>0.016636</td>\n      <td>0.000082</td>\n      <td>396.000000</td>\n      <td>441.00000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1.0</td>\n      <td>33.000000</td>\n      <td>-3.289231</td>\n      <td>-8.909548</td>\n      <td>0.184834</td>\n      <td>0.164710</td>\n      <td>-0.000043</td>\n      <td>1.076654</td>\n      <td>1.076539</td>\n      <td>0.000205</td>\n      <td>14.282609</td>\n      <td>0.016990</td>\n      <td>0.000222</td>\n      <td>602.000000</td>\n      <td>650.00000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.0</td>\n      <td>45.000000</td>\n      <td>-0.639437</td>\n      <td>-5.993006</td>\n      <td>1.246655</td>\n      <td>3.428991</td>\n      <td>0.002369</td>\n      <td>1.197957</td>\n      <td>1.539054</td>\n      <td>1.817914</td>\n      <td>14.772992</td>\n      <td>0.019886</td>\n      <td>0.017966</td>\n      <td>1176.000000</td>\n      <td>1221.00000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  cell_key  cell_batch  cell_num  minimum_dQ_window  variance_dQ_window  \\\n0     b1c0           1         0          -6.581155          -11.155981   \n1     b1c0           1         0          -4.355252          -12.001520   \n2     b1c0           1         0          -0.639437           -7.331691   \n3     b1c0           1         0          -4.803327          -12.056189   \n4     b1c0           1         0          -4.361737          -11.800815   \n\n   skewness_dQ_window  kurtosis_dQ_window  slope_lin_fit_window  \\\n0            0.605746            1.831074         -5.664953e-04   \n1            0.064097            0.485570         -8.211510e-04   \n2            0.204566           -1.054242         -1.063728e-03   \n3           -0.225991            0.029918          1.280844e-05   \n4           -0.059717           -0.917141         -9.811717e-07   \n\n   intercept_lin_fit_window  discharge_capacity_1  \\\n0                  1.098772              1.070689   \n1                  1.105443              1.073992   \n2                  1.111560              1.539054   \n3                  1.076131              1.072405   \n4                  1.076550              1.076654   \n\n   diff_discharge_capacity_max_1  mean_discharge_time  minimum_IR_window  \\\n0                       0.468365            14.437256           0.016560   \n1                       0.465063            14.443377           0.016560   \n2                       0.000000            14.446563           0.016539   \n3                       0.004677            14.466869           0.016444   \n4                       0.000429            14.466661           0.016444   \n\n   diff_IR_window  target_remaining  target_current  target_classifier  \n0       -0.000113            1140.0            50.0                1.0  \n1        0.000008            1135.0            55.0                1.0  \n2       -0.002412            1130.0            60.0                1.0  \n3        0.000008            1125.0            65.0                1.0  \n4        0.000029            1120.0            70.0                1.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cell_key</th>\n      <th>cell_batch</th>\n      <th>cell_num</th>\n      <th>minimum_dQ_window</th>\n      <th>variance_dQ_window</th>\n      <th>skewness_dQ_window</th>\n      <th>kurtosis_dQ_window</th>\n      <th>slope_lin_fit_window</th>\n      <th>intercept_lin_fit_window</th>\n      <th>discharge_capacity_1</th>\n      <th>diff_discharge_capacity_max_1</th>\n      <th>mean_discharge_time</th>\n      <th>minimum_IR_window</th>\n      <th>diff_IR_window</th>\n      <th>target_remaining</th>\n      <th>target_current</th>\n      <th>target_classifier</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>b1c0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>-6.581155</td>\n      <td>-11.155981</td>\n      <td>0.605746</td>\n      <td>1.831074</td>\n      <td>-5.664953e-04</td>\n      <td>1.098772</td>\n      <td>1.070689</td>\n      <td>0.468365</td>\n      <td>14.437256</td>\n      <td>0.016560</td>\n      <td>-0.000113</td>\n      <td>1140.0</td>\n      <td>50.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>b1c0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>-4.355252</td>\n      <td>-12.001520</td>\n      <td>0.064097</td>\n      <td>0.485570</td>\n      <td>-8.211510e-04</td>\n      <td>1.105443</td>\n      <td>1.073992</td>\n      <td>0.465063</td>\n      <td>14.443377</td>\n      <td>0.016560</td>\n      <td>0.000008</td>\n      <td>1135.0</td>\n      <td>55.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>b1c0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>-0.639437</td>\n      <td>-7.331691</td>\n      <td>0.204566</td>\n      <td>-1.054242</td>\n      <td>-1.063728e-03</td>\n      <td>1.111560</td>\n      <td>1.539054</td>\n      <td>0.000000</td>\n      <td>14.446563</td>\n      <td>0.016539</td>\n      <td>-0.002412</td>\n      <td>1130.0</td>\n      <td>60.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>b1c0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>-4.803327</td>\n      <td>-12.056189</td>\n      <td>-0.225991</td>\n      <td>0.029918</td>\n      <td>1.280844e-05</td>\n      <td>1.076131</td>\n      <td>1.072405</td>\n      <td>0.004677</td>\n      <td>14.466869</td>\n      <td>0.016444</td>\n      <td>0.000008</td>\n      <td>1125.0</td>\n      <td>65.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>b1c0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>-4.361737</td>\n      <td>-11.800815</td>\n      <td>-0.059717</td>\n      <td>-0.917141</td>\n      <td>-9.811717e-07</td>\n      <td>1.076550</td>\n      <td>1.076654</td>\n      <td>0.000429</td>\n      <td>14.466661</td>\n      <td>0.016444</td>\n      <td>0.000029</td>\n      <td>1120.0</td>\n      <td>70.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Preprocessing and feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Int64Index([], dtype='int64')"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_1_2_keys = df['cell_key'][df['cell_batch']!=3].unique()\n",
    "train_keys = batch_1_2_keys[1::2]\n",
    "test_keys = batch_1_2_keys[0::2]\n",
    "train_ind = df[df['cell_key'].isin(train_keys)].index\n",
    "test_ind = df[df['cell_key'].isin(test_keys)].index\n",
    "secondary_test_ind = df[df['cell_batch']==3].index\n",
    "\n",
    "splits = [train_ind, test_ind, secondary_test_ind]\n",
    "# secondary_test_ind currently empty, need to fix this code.\n",
    "splits.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define feature and target columns for regression models\n",
    "\n",
    "varmod_features = [\"variance_dQ_window\"]\n",
    "dismod_features = [\n",
    "    \"variance_dQ_window\",\n",
    "    \"minimum_dQ_window\",\n",
    "    \"skewness_dQ_window\",\n",
    "    \"kurtosis_dQ_window\",\n",
    "    \"discharge_capacity_1\",\n",
    "    \"diff_discharge_capacity_max_1\",\n",
    "]\n",
    "fullmod_features = [\n",
    "    \"minimum_dQ_window\",\n",
    "    \"variance_dQ_window\",\n",
    "    \"slope_lin_fit_window\",\n",
    "    \"intercept_lin_fit_window\",\n",
    "    \"discharge_capacity_1\",\n",
    "    \"mean_discharge_time\",\n",
    "    \"minimum_IR_window\",\n",
    "    \"diff_IR_window\",\n",
    "]\n",
    "targetmod = [\"target_remaining\"]  # , \"target_current\"\n",
    "\n",
    "# Define feature and target columns for classifiers\n",
    "\n",
    "varclf_features = [\"variance_dQ_window\"]\n",
    "fullclf_features = [\n",
    "    \"minimum_dQ_window\",\n",
    "    \"variance_dQ_window\",\n",
    "    \"discharge_capacity_1\",\n",
    "    \"diff_IR_window\",\n",
    "]\n",
    "targetclf = [\"target_classifier\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_split(data, features, target, split):\n",
    "    X = data.iloc[split,:].loc[:,features]\n",
    "    y = data.iloc[split,:].loc[:,target]\n",
    "    return X, y\n",
    "\n",
    "def eval_model(model, data, features, target, splits):\n",
    "    ''' TODO: Redo splits allocations. Last index set is empty and don't trust the way in which sets were split, not\n",
    "    very scientific. '''\n",
    "    mse = list()\n",
    "    mae = list()\n",
    "    mpe = list()\n",
    "    for split in splits:\n",
    "        #print(split.astype(str))\n",
    "        print(split)\n",
    "        X, y = get_split(data, features, target, split)\n",
    "        pred = model.predict(X)\n",
    "        mse.append(mean_squared_error(pred, y))\n",
    "        mae.append(float(np.mean(np.abs(y-pred.reshape(-1,1)))))\n",
    "        mpe.append(float(np.mean(np.abs((y - pred.reshape(-1,1))) / y * 100)))\n",
    "    return mse, mae, mpe\n",
    "\n",
    "def eval_classifier(model, data, features, target, splits):\n",
    "    acc = list()    \n",
    "    for split in splits:\n",
    "        X, y = get_split(data, features, target, split)\n",
    "        pred = model.predict(X)\n",
    "        acc.append(accuracy_score(pred, y.values.ravel()))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Variance Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic Net: 0.31910735222047126\n",
      "Linear Regression: 0.3191073522205553\n",
      "Int64Index([ 228,  229,  230,  231,  232,  233,  234,  235,  236,  237,\n",
      "            ...\n",
      "            6354, 6355, 6356, 6357, 6358, 6359, 6360, 6361, 6362, 6363],\n",
      "           dtype='int64', length=3123)\n",
      "Int64Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n",
      "            ...\n",
      "            6464, 6465, 6466, 6467, 6468, 6469, 6470, 6471, 6472, 6473],\n",
      "           dtype='int64', length=3351)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\model_selection\\_search.py:926: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.best_estimator_.fit(X, y, **fit_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: 0.36307435911524233\n",
      "Int64Index([ 228,  229,  230,  231,  232,  233,  234,  235,  236,  237,\n",
      "            ...\n",
      "            6354, 6355, 6356, 6357, 6358, 6359, 6360, 6361, 6362, 6363],\n",
      "           dtype='int64', length=3123)\n",
      "Int64Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n",
      "            ...\n",
      "            6464, 6465, 6466, 6467, 6468, 6469, 6470, 6471, 6472, 6473],\n",
      "           dtype='int64', length=3351)\n",
      "varmod_rf_mse [43386.12771826679, 52694.903597590775]\n",
      "varmod_rf_mae [166.87005638157137, 183.03692475141034]\n",
      "varmod_rf_mpe [134.38089696330604, 153.1970658424267]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Train Elastic net\n",
    "x_train, y_train = get_split(df, varmod_features, targetmod, train_ind)\n",
    "\n",
    "alphas = np.linspace(0.0001,1,30)\n",
    "parameters = {\n",
    "    \"alpha\": alphas,\n",
    "    \"l1_ratio\": [0.01, 0.25, 0.5, 0.75, 1.]\n",
    "}\n",
    "enet = ElasticNet(random_state=54)\n",
    "regr = GridSearchCV(enet, parameters, cv=4)\n",
    "print(\"Elastic Net: %s\" % regr.fit(x_train, y_train).score(x_train, y_train))\n",
    "\n",
    "\"\"\"\n",
    "Because an elastic net with alpha = 0 is technically a linear regression\n",
    "and elastic net produces inaccuracies with a small alpha,\n",
    "we also train a linear regression model.\n",
    "Linear regression performs slighty better at RMSE,\n",
    "Elastic net performs slightly better at MPE.\n",
    "We decide to take the linear regression scores.\n",
    "\"\"\"\n",
    "lin_reg = LinearRegression()\n",
    "print(\"Linear Regression: %s\" % lin_reg.fit(x_train, y_train).score(x_train, y_train))\n",
    "\n",
    "varmod_mse, varmod_mae, varmod_mpe = eval_model(lin_reg, df, varmod_features, targetmod, splits)\n",
    "\n",
    "\n",
    "# Add Random Forest\n",
    "rf_params = {\n",
    "    \"max_depth\": [2, 3],\n",
    "    \"n_estimators\": [10, 100]\n",
    "}\n",
    "rfst = RandomForestRegressor(random_state=54)\n",
    "rfst_grid = GridSearchCV(rfst, rf_params, cv=4)\n",
    "print(\"Random Forest: %s\" % rfst_grid.fit(x_train, y_train).score(x_train, y_train))\n",
    "\n",
    "varmod_rf_mse, varmod_rf_mae, varmod_rf_mpe = eval_model(rfst_grid, df, varmod_features, targetmod, splits)\n",
    "print('varmod_rf_mse', varmod_rf_mse)\n",
    "print('varmod_rf_mae', varmod_rf_mae)\n",
    "print('varmod_rf_mpe', varmod_mpe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Discharge Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic Net: 0.623430876917282\n",
      "Int64Index([ 228,  229,  230,  231,  232,  233,  234,  235,  236,  237,\n",
      "            ...\n",
      "            6354, 6355, 6356, 6357, 6358, 6359, 6360, 6361, 6362, 6363],\n",
      "           dtype='int64', length=3123)\n",
      "Int64Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n",
      "            ...\n",
      "            6464, 6465, 6466, 6467, 6468, 6469, 6470, 6471, 6472, 6473],\n",
      "           dtype='int64', length=3351)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Train Elastic net\n",
    "x_train, y_train = get_split(df, dismod_features, targetmod, train_ind)\n",
    "\n",
    "alphas = np.linspace(0.1,1,20)\n",
    "parameters = {\n",
    "    \"alpha\": alphas,\n",
    "    \"l1_ratio\": [0.01, 0.25, 0.5, 0.75, 1.]\n",
    "}\n",
    "enet = ElasticNet(random_state=54)\n",
    "regr = GridSearchCV(enet, parameters, cv=4)\n",
    "print(\"Elastic Net: %s\" % regr.fit(x_train, y_train).score(x_train, y_train))\n",
    "\n",
    "dismod_mse, dismod_mae, dismod_mpe = eval_model(regr, df, dismod_features, targetmod, splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.802e+06, tolerance: 1.185e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.027e+07, tolerance: 1.663e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.363e+07, tolerance: 1.737e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.380e+05, tolerance: 1.723e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic Net: 0.7520921839465965\n",
      "Int64Index([ 228,  229,  230,  231,  232,  233,  234,  235,  236,  237,\n",
      "            ...\n",
      "            6354, 6355, 6356, 6357, 6358, 6359, 6360, 6361, 6362, 6363],\n",
      "           dtype='int64', length=3123)\n",
      "Int64Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n",
      "            ...\n",
      "            6464, 6465, 6466, 6467, 6468, 6469, 6470, 6471, 6472, 6473],\n",
      "           dtype='int64', length=3351)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.222e+07, tolerance: 2.127e+04\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Train Elastic net model\n",
    "# raising the alpha minimum to 0.59 silences the convergence warnings,\n",
    "# but decreases the score significantly - what's wrong here? \n",
    "\n",
    "x_train, y_train = get_split(df, fullmod_features, targetmod, train_ind)\n",
    "\n",
    "alphas = np.linspace(0.001,1,20)\n",
    "parameters = {\n",
    "    \"alpha\": alphas,\n",
    "    \"l1_ratio\": [0.001, 0.75, 1.]\n",
    "}\n",
    "enet = ElasticNet(random_state=54)\n",
    "regr = GridSearchCV(enet, parameters, cv=4)\n",
    "print(\"Elastic Net: %s\" % regr.fit(x_train, y_train).score(x_train, y_train))\n",
    "\n",
    "fullmod_mse, fullmod_mae, fullmod_mpe = eval_model(regr, df, fullmod_features, targetmod, splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Evaluate all linear regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "             Model  MAE - Train  MAE - Primary test   MSE - Train  \\\n0   Variance model   176.059119          192.360618  46381.074152   \n1  Discharge model   120.746249          137.240702  25651.151438   \n2       Full model    94.570483          113.500848  16886.995089   \n\n   MSE - Primary test  MPE - Train  MPE - Primary test  \n0        56627.238495   134.380897          153.197066  \n1        33066.161719   103.098096          118.896832  \n2        24651.316252   113.586260          133.179104  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>MAE - Train</th>\n      <th>MAE - Primary test</th>\n      <th>MSE - Train</th>\n      <th>MSE - Primary test</th>\n      <th>MPE - Train</th>\n      <th>MPE - Primary test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Variance model</td>\n      <td>176.059119</td>\n      <td>192.360618</td>\n      <td>46381.074152</td>\n      <td>56627.238495</td>\n      <td>134.380897</td>\n      <td>153.197066</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Discharge model</td>\n      <td>120.746249</td>\n      <td>137.240702</td>\n      <td>25651.151438</td>\n      <td>33066.161719</td>\n      <td>103.098096</td>\n      <td>118.896832</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Full model</td>\n      <td>94.570483</td>\n      <td>113.500848</td>\n      <td>16886.995089</td>\n      <td>24651.316252</td>\n      <td>113.586260</td>\n      <td>133.179104</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\"Model\":[\"Variance model\", \"Discharge model\", \"Full model\"],\n",
    "              \"MAE - Train\": [varmod_mae[0],dismod_mae[0],fullmod_mae[0]],\n",
    "              \"MAE - Primary test\": [varmod_mae[1],dismod_mae[1],fullmod_mae[1]],\n",
    "              #\"MAE - Secondary test\": [varmod_mae[2],dismod_mae[2],fullmod_mae[2]],\n",
    "              \"MSE - Train\": [varmod_mse[0],dismod_mse[0],fullmod_mse[0]],\n",
    "              \"MSE - Primary test\": [varmod_mse[1],dismod_mse[1],fullmod_mse[1]],\n",
    "              #\"MSE - Secondary test\": [varmod_mse[2],dismod_mse[2],fullmod_mse[2]],\n",
    "              \"MPE - Train\": [varmod_mpe[0],dismod_mpe[0],fullmod_mpe[0]],\n",
    "              \"MPE - Primary test\": [varmod_mpe[1],dismod_mpe[1],fullmod_mpe[1]],\n",
    "              #\"MPE - Secondary test\": [varmod_mpe[2],dismod_mpe[2],fullmod_mpe[2]]\n",
    "             })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Variance Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logreg: 0.7431956452129362\n"
     ]
    }
   ],
   "source": [
    "# Train Logistic Regression\n",
    "x_train, y_train = get_split(df, varclf_features, targetclf, train_ind)\n",
    "\n",
    "parameters = {\"C\": [0.01,0.1,0.5,0.75,1]}\n",
    "\n",
    "logreg = LogisticRegression(solver=\"liblinear\", random_state=54)\n",
    "clf = GridSearchCV(logreg, parameters, cv=4)\n",
    "print(\"Logreg: %s\" % clf.fit(x_train, y_train.values.ravel()).score(x_train, y_train.values.ravel()))\n",
    "\n",
    "varclf_acc = eval_classifier(clf, df, varclf_features, targetclf, splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Full Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logreg: 0.7454370797310279\n"
     ]
    }
   ],
   "source": [
    "# Train Logistic Regression\n",
    "# Why is the full classifier worse than the variance classifier?\n",
    "x_train, y_train = get_split(df, fullclf_features, targetclf, train_ind)\n",
    "\n",
    "parameters = {\"C\": [0.01,0.1,0.5,0.75,1]}\n",
    "\n",
    "logreg = LogisticRegression(solver=\"liblinear\", random_state=54)\n",
    "clf = GridSearchCV(logreg, parameters, cv=4)\n",
    "print(\"Logreg: %s\" % clf.fit(x_train, y_train.values.ravel()).score(x_train, y_train.values.ravel()))\n",
    "\n",
    "fullclf_acc = eval_classifier(clf, df, fullclf_features, targetclf, splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Evaluate all classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "            Classifier  Acc - Train  Acc - Primary test\n0  Variance classifier     0.743196            0.734706\n1      Full classifier     0.745437            0.741868",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Classifier</th>\n      <th>Acc - Train</th>\n      <th>Acc - Primary test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Variance classifier</td>\n      <td>0.743196</td>\n      <td>0.734706</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Full classifier</td>\n      <td>0.745437</td>\n      <td>0.741868</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\"Classifier\":[\"Variance classifier\", \"Full classifier\"],\n",
    "              \"Acc - Train\": [varclf_acc[0],fullclf_acc[0]],\n",
    "              \"Acc - Primary test\": [varclf_acc[1],fullclf_acc[1]],})\n",
    "              #\"Acc - Secondary test\": [varclf_acc[2],fullclf_acc[2]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}