{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Data format</b><br>\n",
    "1 Row = 1 Example = 1 Cycle<br>\n",
    "Each cycle so far has:\n",
    " - Qdlin (1000,1)\n",
    " - Tdlin (1000,1)\n",
    " - Cdlin (1000,1) (WIP)\n",
    " - discharge_time (1,) (WIP)\n",
    " - IR (1,)\n",
    " - remaining_cycle_life (1,) <- target\n",
    " \n",
    "For every cell we create one TFRecord file where each row represents data for one cycle. Before model training we\n",
    "read data from all files, create one dataset and feed it to the model. <br>Below are two different ways of\n",
    "structuring this data while reading and writing. Depending on what input the final model needs, we can chose the\n",
    "appropriate dataset design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: C:\\Users\\PC-1\\Documents\\GitHub\\ProjectsPrivate\\coin_cell_battery\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.train import FloatList, Int64List, Feature, FeatureList, FeatureLists, SequenceExample, Features, Example\n",
    "from tensorflow.feature_column import numeric_column, make_parse_example_spec\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Conv1D, Flatten, TimeDistributed, Activation\n",
    "\n",
    "# change working directory to base, to make all imports and file paths work\n",
    "import os\n",
    "#os.chdir(os.pardir)\n",
    "os.chdir(r\"C:\\Users\\PC-1\\Documents\\GitHub\\ProjectsPrivate\\coin_cell_battery\")\n",
    "print(\"Current directory: %s\" % os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load only batch1 for testing\n",
    "path1 = Path(\"./data/external/batch1.pkl\")\n",
    "batch1 = pickle.load(open(path1, 'rb'))\n",
    "\n",
    "# remove batteries that do not reach 80% capacity\n",
    "del batch1['b1c8']\n",
    "del batch1['b1c10']\n",
    "del batch1['b1c12']\n",
    "del batch1['b1c13']\n",
    "del batch1['b1c22']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) With tf.train.Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "see Hands-On Machine Learning pp.416\n",
    "\n",
    "1. The get_cycle_features function fetches all features and targets from \n",
    "the batch1 file and convert to \"Example\" objects. Every Example contains \n",
    "data from one charging cycle.\n",
    "\n",
    "2. Create a \"Data/tfrecords\" directory.\n",
    "\n",
    "3. For each cell create a tfrecord file with the naming convention \"b1c0.tfrecord\".\n",
    "The SerializeToString method creates binary data out of the Example objects that can\n",
    "be read natively in TensorFlow.\n",
    "\"\"\"\n",
    "\n",
    "def get_cycle_example(cell, idx):\n",
    "    cycle_example = Example(\n",
    "        features=Features(\n",
    "            feature={\n",
    "                \"IR\": Feature(float_list=FloatList(value=[batch1[cell][\"summary\"][\"IR\"][idx]])),\n",
    "                \"Qdlin\": Feature(float_list=FloatList(value=batch1[cell][\"cycles\"][str(idx)][\"Qdlin\"])),\n",
    "                \"Tdlin\": Feature(float_list=FloatList(value=batch1[cell][\"cycles\"][str(idx)][\"Tdlin\"])),\n",
    "                \"Remaining_cycles\": Feature(int64_list=Int64List(value=[int(batch1[cell][\"cycle_life\"]-idx)]))\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    return cycle_example\n",
    "\n",
    "data_dir = \"Data/tfrecords/\"\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "    \n",
    "for cell in batch1:\n",
    "    filename = os.path.join(data_dir + cell + \".tfrecord\")\n",
    "    with tf.io.TFRecordWriter(filename) as f:\n",
    "        num_cycles = int(batch1[cell][\"cycle_life\"])-1\n",
    "        for cycle in range(num_cycles):\n",
    "            cycle_to_write = get_cycle_example(cell, cycle)\n",
    "            f.write(cycle_to_write.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define columns for our dataset\n",
    "ir = numeric_column(\"IR\", shape=[])\n",
    "qdlin = numeric_column(\"Qdlin\", shape=[1000, 1])\n",
    "tdlin = numeric_column(\"Tdlin\", shape=[1000, 1])\n",
    "remaining_cycles = numeric_column(\"Remaining_cycles\", shape=[], dtype=tf.int64)\n",
    "\n",
    "columns = [ir, qdlin, tdlin, remaining_cycles]\n",
    "example_spec = make_parse_example_spec(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'No files matched pattern: ./data/processed/tfrecords/b1c0.tfrecord, ./data/processed/tfrecords/b1c1.tfrecord, ./data/processed/tfrecords/b1c2.tfrecord, ./data/processed/tfrecords/b1c3.tfrecord, ./data/processed/tfrecords/b1c4.tfrecord, ./data/processed/tfrecords/b1c5.tfrecord, ./data/processed/tfrecords/b1c6.tfrecord, ./data/processed/tfrecords/b1c7.tfrecord, ./data/processed/tfrecords/b1c9.tfrecord, ./data/processed/tfrecords/b1c11.tfrecord, ./data/processed/tfrecords/b1c14.tfrecord, ./data/processed/tfrecords/b1c15.tfrecord, ./data/processed/tfrecords/b1c16.tfrecord, ./data/processed/tfrecords/b1c17.tfrecord, ./data/processed/tfrecords/b1c18.tfrecord, ./data/processed/tfrecords/b1c19.tfrecord, ./data/processed/tfrecords/b1c20.tfrecord, ./data/processed/tfrecords/b1c21.tfrecord, ./data/processed/tfrecords/b1c23.tfrecord, ./data/processed/tfrecords/b1c24.tfrecord, ./data/processed/tfrecords/b1c25.tfrecord, ./data/processed/tfrecords/b1c26.tfrecord, ./data/processed/tfrecords/b1c27.tfrecord, ./data/processed/tfrecords/b1c28.tfrecord, ./data/processed/tfrecords/b1c29.tfrecord, ./data/processed/tfrecords/b1c30.tfrecord, ./data/processed/tfrecords/b1c31.tfrecord, ./data/processed/tfrecords/b1c32.tfrecord, ./data/processed/tfrecords/b1c33.tfrecord, ./data/processed/tfrecords/b1c34.tfrecord, ./data/processed/tfrecords/b1c35.tfrecord, ./data/processed/tfrecords/b1c36.tfrecord, ./data/processed/tfrecords/b1c37.tfrecord, ./data/processed/tfrecords/b1c38.tfrecord, ./data/processed/tfrecords/b1c39.tfrecord, ./data/processed/tfrecords/b1c40.tfrecord, ./data/processed/tfrecords/b1c41.tfrecord, ./data/processed/tfrecords/b1c42.tfrecord, ./data/processed/tfrecords/b1c43.tfrecord, ./data/processed/tfrecords/b1c44.tfrecord, ./data/processed/tfrecords/b1c45.tfrecord'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mInvalidArgumentError\u001B[0m                      Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-7-2fa982d2f1db>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     63\u001B[0m \u001B[1;31m# define files to read from and store in a list_files object\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     64\u001B[0m \u001B[0mfilepaths\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"./data/processed/tfrecords/\"\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mcell\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;34m\".tfrecord\"\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mcell\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mbatch1_keys\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 65\u001B[1;33m \u001B[0mfilepath_dataset\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDataset\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlist_files\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepaths\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     66\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     67\u001B[0m \u001B[0mdataset\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfilepath_dataset\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minterleave\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mread_tfrecords\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcycle_length\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m4\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnum_parallel_calls\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m4\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001B[0m in \u001B[0;36mlist_files\u001B[1;34m(file_pattern, shuffle, seed)\u001B[0m\n\u001B[0;32m   1222\u001B[0m           string_ops.reduce_join(file_pattern, separator=\", \"), name=\"message\")\n\u001B[0;32m   1223\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1224\u001B[1;33m       assert_not_empty = control_flow_ops.Assert(\n\u001B[0m\u001B[0;32m   1225\u001B[0m           condition, [message], summarize=1, name=\"assert_not_empty\")\n\u001B[0;32m   1226\u001B[0m       \u001B[1;32mwith\u001B[0m \u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcontrol_dependencies\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0massert_not_empty\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    199\u001B[0m     \u001B[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    200\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 201\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mtarget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    202\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mTypeError\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    203\u001B[0m       \u001B[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\Lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\u001B[0m in \u001B[0;36mwrapped\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    245\u001B[0m     \u001B[1;34m\"\"\"Decorates the input function.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    246\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mwrapped\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 247\u001B[1;33m       return _add_should_use_warning(fn(*args, **kwargs),\n\u001B[0m\u001B[0;32m    248\u001B[0m                                      \u001B[0mwarn_in_eager\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mwarn_in_eager\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    249\u001B[0m                                      error_in_function=error_in_function)\n",
      "\u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\Lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001B[0m in \u001B[0;36mAssert\u001B[1;34m(condition, data, summarize, name)\u001B[0m\n\u001B[0;32m    152\u001B[0m       \u001B[0mxs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconvert_n_to_tensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    153\u001B[0m       \u001B[0mdata_str\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0m_summarize_eager\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msummarize\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mx\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mxs\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 154\u001B[1;33m       raise errors.InvalidArgumentError(\n\u001B[0m\u001B[0;32m    155\u001B[0m           \u001B[0mnode_def\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    156\u001B[0m           \u001B[0mop\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mInvalidArgumentError\u001B[0m: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'No files matched pattern: ./data/processed/tfrecords/b1c0.tfrecord, ./data/processed/tfrecords/b1c1.tfrecord, ./data/processed/tfrecords/b1c2.tfrecord, ./data/processed/tfrecords/b1c3.tfrecord, ./data/processed/tfrecords/b1c4.tfrecord, ./data/processed/tfrecords/b1c5.tfrecord, ./data/processed/tfrecords/b1c6.tfrecord, ./data/processed/tfrecords/b1c7.tfrecord, ./data/processed/tfrecords/b1c9.tfrecord, ./data/processed/tfrecords/b1c11.tfrecord, ./data/processed/tfrecords/b1c14.tfrecord, ./data/processed/tfrecords/b1c15.tfrecord, ./data/processed/tfrecords/b1c16.tfrecord, ./data/processed/tfrecords/b1c17.tfrecord, ./data/processed/tfrecords/b1c18.tfrecord, ./data/processed/tfrecords/b1c19.tfrecord, ./data/processed/tfrecords/b1c20.tfrecord, ./data/processed/tfrecords/b1c21.tfrecord, ./data/processed/tfrecords/b1c23.tfrecord, ./data/processed/tfrecords/b1c24.tfrecord, ./data/processed/tfrecords/b1c25.tfrecord, ./data/processed/tfrecords/b1c26.tfrecord, ./data/processed/tfrecords/b1c27.tfrecord, ./data/processed/tfrecords/b1c28.tfrecord, ./data/processed/tfrecords/b1c29.tfrecord, ./data/processed/tfrecords/b1c30.tfrecord, ./data/processed/tfrecords/b1c31.tfrecord, ./data/processed/tfrecords/b1c32.tfrecord, ./data/processed/tfrecords/b1c33.tfrecord, ./data/processed/tfrecords/b1c34.tfrecord, ./data/processed/tfrecords/b1c35.tfrecord, ./data/processed/tfrecords/b1c36.tfrecord, ./data/processed/tfrecords/b1c37.tfrecord, ./data/processed/tfrecords/b1c38.tfrecord, ./data/processed/tfrecords/b1c39.tfrecord, ./data/processed/tfrecords/b1c40.tfrecord, ./data/processed/tfrecords/b1c41.tfrecord, ./data/processed/tfrecords/b1c42.tfrecord, ./data/processed/tfrecords/b1c43.tfrecord, ./data/processed/tfrecords/b1c44.tfrecord, ./data/processed/tfrecords/b1c45.tfrecord'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Reading the remaining code from bottom to top:\n",
    "\n",
    "When writing to TFrecord we created one file for each cell. Now we merge the\n",
    "data back into one dataset and prepare it to be fed directly into a model.\n",
    "\n",
    "The interleave() method will create a dataset that pulls 4 file paths from the\n",
    "filepath_dataset and for each one calls the function \"read_tfrecords\". It will then\n",
    "cycle through these 4 datasets, reading one line at a time from each until all datasets\n",
    "are out of items. Then it gets the next 4 file paths from the filepath_dataset and\n",
    "interleaves them the same way, and so on until it runs out of file paths. \n",
    "Note: Even with parallel calls specified, data within batches is still sequential.\n",
    "\n",
    "The read_tfrecords() function reads a file, skipping the first row which in our case\n",
    "is 0/NaN most of the time. It then loops over each example/row in the dataset and\n",
    "calls the parse_feature function. Then it batches the dataset, so it always feeds\n",
    "multiple examples at the same time, and then shuffles the batches. It is important \n",
    "that we batch before shuffling, so the examples within the batches stay in order.\n",
    "\n",
    "The parse_features function takes an example and converts it from binary/message format\n",
    "into a more readable format. The make_parse_example_spec generates a feature mapping \n",
    "according to the columns we defined. To be able to feed the dataset directly into a\n",
    "Tensorflow model later on, we need to split the data into examples and targets (i.e. X and y).\n",
    "\"\"\"\n",
    "# define variables\n",
    "batch1_keys = ['b1c0', 'b1c1', 'b1c2', 'b1c3', 'b1c4', 'b1c5', 'b1c6', 'b1c7', 'b1c9', 'b1c11', 'b1c14', 'b1c15', 'b1c16', 'b1c17', 'b1c18', 'b1c19', 'b1c20', 'b1c21', 'b1c23', 'b1c24', 'b1c25', 'b1c26', 'b1c27', 'b1c28', 'b1c29', 'b1c30', 'b1c31', 'b1c32', 'b1c33', 'b1c34', 'b1c35', 'b1c36', 'b1c37', 'b1c38', 'b1c39', 'b1c40', 'b1c41', 'b1c42', 'b1c43', 'b1c44', 'b1c45']\n",
    "window_size = 5\n",
    "shift = 1\n",
    "stride = 1\n",
    "\n",
    "def parse_features(example_proto):\n",
    "    examples = tf.io.parse_single_example(example_proto, example_spec)\n",
    "    targets = examples.pop(\"Remaining_cycles\")\n",
    "    return examples, targets\n",
    "\n",
    "def flatten_windows(features, target):\n",
    "    feat1 = features[\"Qdlin\"].batch(window_size)    \n",
    "    target_flat = target.skip(window_size-1)\n",
    "    return tf.data.Dataset.zip((feat1, target_flat))\n",
    "\n",
    "def flatten_windows_all_features(features, target):\n",
    "    \"\"\"\n",
    "    This method returns all features instead of just one, but \n",
    "    messes up the input shape. This method is not used at the\n",
    "    moment, but after we  figure out how to feed every feature\n",
    "    to the model, it could replace 'flatten_windows()'.\n",
    "    \"\"\"\n",
    "    feat1 = features[\"IR\"].batch(window_size)\n",
    "    feat2 = features[\"Qdlin\"].batch(window_size)\n",
    "    feat3 = features[\"Tdlin\"].batch(window_size)\n",
    "    features = tf.data.Dataset.zip((feat1, feat2, feat3))\n",
    "    target_flat = target.skip(window_size-1)\n",
    "    return tf.data.Dataset.zip((features, target_flat))\n",
    "\n",
    "def read_tfrecords(file):\n",
    "    dataset = tf.data.TFRecordDataset(file).skip(1) # skip can be removed when we have clean data\n",
    "    dataset = dataset.map(parse_features)\n",
    "    dataset = dataset.window(size=window_size, shift=shift, stride=stride, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(flatten_windows)\n",
    "    dataset = dataset.shuffle(1000).batch(10).prefetch(1) # prefetch is only relevant for CPU to GPU pipelines, see Hands-On ML p.411\n",
    "    return dataset\n",
    "\n",
    "# define files to read from and store in a list_files object\n",
    "filepaths = [os.path.join(\"./data/processed/tfrecords/\" + cell + \".tfrecord\") for cell in batch1_keys]\n",
    "filepath_dataset = tf.data.Dataset.list_files(filepaths)\n",
    "\n",
    "dataset = filepath_dataset.interleave(read_tfrecords, cycle_length=4, num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To feed it into a CNN with a Timedistibuted layer, we need this input:<br>\n",
    "Examples: [ batch_size, window_size, steps, input_dim ]<br>\n",
    "e.g.: [ 10, 5, 1000, 1 ]<br>\n",
    "Laels labels are contained in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature, target in dataset.take(1):\n",
    "    print(\"Input shape: %s\" % [*feature.shape])\n",
    "    print(\"Target: %s\" % target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset compatibility with a CNN + LSTM model layout\n",
    "steps = 1000\n",
    "input_dim = 1\n",
    "\n",
    "model = Sequential()\n",
    "# define CNN model\n",
    "model.add(TimeDistributed(Conv1D(filters=1, kernel_size=3, activation='relu'), input_shape=(window_size,steps,input_dim)))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "# define LSTM model\n",
    "model.add(LSTM(50, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss=\"mean_squared_error\", optimizer='adam')\n",
    "model.fit(dataset, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) With tf.train.SequenceExample (and FeatureLists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Same as in 1), but with an additional layer that sorts features into two categories \n",
    " - \"context\": \"IR\", \"Remaining_cycles\", \"Discharge_time\" (WIP) \n",
    " - \"details\": \"Qdlin\", \"Tdlin\"\n",
    "This stores Qdlin and Tdlin in one matrix which we use access more easily as input,\n",
    "but we lose their name handle. \n",
    "\"\"\"\n",
    "\n",
    "def get_sequence_example(cell, idx):\n",
    "    ir = Feature(float_list=FloatList(value=[batch1[cell][\"summary\"][\"IR\"][idx]]))\n",
    "    qdlin = Feature(float_list=FloatList(value=batch1[cell][\"cycles\"][str(idx)][\"Qdlin\"]))\n",
    "    tdlin = Feature(float_list=FloatList(value=batch1[cell][\"cycles\"][str(idx)][\"Tdlin\"]))\n",
    "    remaining_cycles = Feature(float_list=FloatList(value=[int(batch1[cell][\"cycle_life\"]-idx)]))\n",
    "\n",
    "    detail_features = FeatureList(feature=[qdlin, tdlin])\n",
    "\n",
    "    cycle_example = SequenceExample(\n",
    "        context = Features(\n",
    "            feature={\"IR\":ir,\n",
    "                     \"Remaining_cycles\": remaining_cycles\n",
    "                    }\n",
    "        ),\n",
    "        feature_lists = FeatureLists(feature_list={\"Details\":detail_features})\n",
    "    )\n",
    "    return cycle_example\n",
    "    \n",
    "data_dir = \"./data/processed/tfrecords_featurelists/\"\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "    \n",
    "for cell in batch1:\n",
    "    filename = os.path.join(data_dir + cell + \".tfrecord\")\n",
    "    with tf.io.TFRecordWriter(filename) as f:\n",
    "            num_cycles = int(batch1[\"b1c0\"][\"cycle_life\"])-1\n",
    "            for cycle in range(num_cycles):\n",
    "                cycle_to_write = get_sequence_example(cell, cycle)\n",
    "                f.write(cycle_to_write.SerializeToString())\n",
    "    break # write only one cell for testing. Remove this to write all cells from batch1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variables\n",
    "batch1_keys = ['b1c0', 'b1c1', 'b1c2', 'b1c3', 'b1c4', 'b1c5', 'b1c6', 'b1c7', 'b1c9', 'b1c11', 'b1c14', 'b1c15', 'b1c16', 'b1c17', 'b1c18', 'b1c19', 'b1c20', 'b1c21', 'b1c23', 'b1c24', 'b1c25', 'b1c26', 'b1c27', 'b1c28', 'b1c29', 'b1c30', 'b1c31', 'b1c32', 'b1c33', 'b1c34', 'b1c35', 'b1c36', 'b1c37', 'b1c38', 'b1c39', 'b1c40', 'b1c41', 'b1c42', 'b1c43', 'b1c44', 'b1c45']\n",
    "window_size = 5\n",
    "shift = 1\n",
    "stride = 1\n",
    "\n",
    "context_feature_description = {\n",
    "    \"IR\": tf.io.FixedLenFeature([], tf.float32),\n",
    "    \"Remaining_cycles\": tf.io.FixedLenFeature([], tf.float32)\n",
    "}\n",
    "\n",
    "sequence_feature_description = {\n",
    "    \"Details\": tf.io.FixedLenSequenceFeature([1000], tf.float32),\n",
    "}\n",
    "    \n",
    "def parse_features(example_proto):\n",
    "    sequence_example = tf.io.parse_single_sequence_example(\n",
    "        example_proto,\n",
    "        context_feature_description,\n",
    "        sequence_feature_description\n",
    "    )\n",
    "    targets = sequence_example[0].pop(\"Remaining_cycles\")\n",
    "    return sequence_example, targets\n",
    "\n",
    "def flatten_windows(features, target):\n",
    "    feat1 = features[1][\"Details\"].batch(window_size)\n",
    "    feat2 = features[0][\"IR\"].batch(window_size)\n",
    "    target_flat = target.skip(window_size-1)\n",
    "    return tf.data.Dataset.zip((feat1, feat2, target_flat))\n",
    "\n",
    "def read_tfrecords(file):\n",
    "    dataset = tf.data.TFRecordDataset(file).skip(1) # skip can be removed when we have clean data\n",
    "    dataset = dataset.map(parse_features)\n",
    "    dataset = dataset.window(size=window_size, shift=shift, stride=stride, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(flatten_windows).batch(8).shuffle(1000)\n",
    "    return dataset\n",
    "\n",
    "filepaths = [os.path.join(\"./data/processed/tfrecords_featurelists/\" + cell + \".tfrecord\") for cell in batch1_keys]\n",
    "filepath_dataset = tf.data.Dataset.list_files(filepaths)\n",
    "\n",
    "dataset = filepath_dataset.interleave(read_tfrecords, cycle_length=4, num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Instead of the standard way to return data - (Features, Target) - \n",
    "this dataset returns (Details, Context, Target). This can be changed\n",
    "in flatten_windows().\n",
    "One way to retain the (Features, Target) structure would be to zip\n",
    "feat1 and feat2 before returning everything. But this would nest \n",
    "the data so it can't be accessed as easily.\n",
    "\"\"\"\n",
    "\n",
    "# The Qdlin/Tdlin matrix needs to be (1000,2) instead of (2,1000)\n",
    "for ex in dataset.take(1):\n",
    "    print(\"Qdlin and Tdlin:\\n %s\" % ex[0].shape)\n",
    "    print(\"IR:\\n %s\" % ex[1].shape)\n",
    "    print(\"Target: \\n%s\" % ex[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shapes don't match yet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-376c4edb",
   "language": "python",
   "display_name": "PyCharm (coin_cell_battery)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}