{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models and Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook reproduces the results from the initial paper, following the given information as closely as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import ElasticNet, LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/external/rebuild_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array(['cell_key', 'minimum_dQ_100_10', 'variance_dQ_100_10',\n       'skewness_dQ_100_10', 'kurtosis_dQ_100_10', 'slope_lin_fit_2_100',\n       'intercept_lin_fit_2_100', 'discharge_capacity_2',\n       'diff_discharge_capacity_max_2', 'mean_charge_time_2_6',\n       'minimum_IR_2_100', 'diff_IR_100_2', 'minimum_dQ_5_4',\n       'variance_dQ_5_4', 'cycle_life', 'cycle_550_clf'], dtype=object)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "       minimum_dQ_100_10  variance_dQ_100_10  skewness_dQ_100_10  \\\ncount         124.000000          124.000000          124.000000   \nmean           -3.309622           -8.856155           -1.798408   \nstd             0.505783            0.937557            1.113668   \nmin            -5.142544          -11.825562           -7.669214   \n25%            -3.615959           -9.463472           -2.307431   \n50%            -3.233402           -8.710854           -1.537733   \n75%            -2.950006           -8.218369           -1.134104   \nmax            -1.984958           -6.296653            0.648827   \n\n       kurtosis_dQ_100_10  slope_lin_fit_2_100  intercept_lin_fit_2_100  \\\ncount          124.000000           124.000000               124.000000   \nmean             0.181338            -0.000033                 1.075610   \nstd              0.209144             0.000116                 0.009705   \nmin             -0.651928            -0.001092                 1.049389   \n25%              0.121294            -0.000020                 1.070539   \n50%              0.207657            -0.000006                 1.076238   \n75%              0.247337             0.000005                 1.082333   \nmax              1.474969             0.000035                 1.101465   \n\n       discharge_capacity_2  diff_discharge_capacity_max_2  \\\ncount            124.000000                     124.000000   \nmean               1.071192                       0.023001   \nstd                0.009061                       0.167760   \nmin                1.042137                       0.000459   \n25%                1.066903                       0.003423   \n50%                1.071413                       0.004417   \n75%                1.077769                       0.005953   \nmax                1.094639                       1.817914   \n\n       mean_charge_time_2_6  minimum_IR_2_100  diff_IR_100_2  minimum_dQ_5_4  \\\ncount            124.000000        124.000000     124.000000      124.000000   \nmean              10.432973          0.014917      -0.000179       -8.054585   \nstd                0.744873          0.004534       0.000436        2.131843   \nmin                8.964706          0.000000      -0.003992      -14.112938   \n25%               10.043770          0.015301      -0.000230       -9.494888   \n50%               10.130237          0.016210      -0.000075       -8.177757   \n75%               10.329087          0.016868      -0.000003       -6.204012   \nmax               13.409150          0.020022       0.000438       -4.260407   \n\n       variance_dQ_5_4   cycle_life  cycle_550_clf  \ncount       124.000000   124.000000     124.000000  \nmean        -14.490349   798.387097       0.653226  \nstd           2.014785   372.742979       0.477874  \nmin         -18.273646   148.000000       0.000000  \n25%         -15.940693   498.750000       0.000000  \n50%         -14.728990   736.500000       1.000000  \n75%         -12.952985   946.500000       1.000000  \nmax          -9.401950  2237.000000       1.000000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>minimum_dQ_100_10</th>\n      <th>variance_dQ_100_10</th>\n      <th>skewness_dQ_100_10</th>\n      <th>kurtosis_dQ_100_10</th>\n      <th>slope_lin_fit_2_100</th>\n      <th>intercept_lin_fit_2_100</th>\n      <th>discharge_capacity_2</th>\n      <th>diff_discharge_capacity_max_2</th>\n      <th>mean_charge_time_2_6</th>\n      <th>minimum_IR_2_100</th>\n      <th>diff_IR_100_2</th>\n      <th>minimum_dQ_5_4</th>\n      <th>variance_dQ_5_4</th>\n      <th>cycle_life</th>\n      <th>cycle_550_clf</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>124.000000</td>\n      <td>124.000000</td>\n      <td>124.000000</td>\n      <td>124.000000</td>\n      <td>124.000000</td>\n      <td>124.000000</td>\n      <td>124.000000</td>\n      <td>124.000000</td>\n      <td>124.000000</td>\n      <td>124.000000</td>\n      <td>124.000000</td>\n      <td>124.000000</td>\n      <td>124.000000</td>\n      <td>124.000000</td>\n      <td>124.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>-3.309622</td>\n      <td>-8.856155</td>\n      <td>-1.798408</td>\n      <td>0.181338</td>\n      <td>-0.000033</td>\n      <td>1.075610</td>\n      <td>1.071192</td>\n      <td>0.023001</td>\n      <td>10.432973</td>\n      <td>0.014917</td>\n      <td>-0.000179</td>\n      <td>-8.054585</td>\n      <td>-14.490349</td>\n      <td>798.387097</td>\n      <td>0.653226</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.505783</td>\n      <td>0.937557</td>\n      <td>1.113668</td>\n      <td>0.209144</td>\n      <td>0.000116</td>\n      <td>0.009705</td>\n      <td>0.009061</td>\n      <td>0.167760</td>\n      <td>0.744873</td>\n      <td>0.004534</td>\n      <td>0.000436</td>\n      <td>2.131843</td>\n      <td>2.014785</td>\n      <td>372.742979</td>\n      <td>0.477874</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-5.142544</td>\n      <td>-11.825562</td>\n      <td>-7.669214</td>\n      <td>-0.651928</td>\n      <td>-0.001092</td>\n      <td>1.049389</td>\n      <td>1.042137</td>\n      <td>0.000459</td>\n      <td>8.964706</td>\n      <td>0.000000</td>\n      <td>-0.003992</td>\n      <td>-14.112938</td>\n      <td>-18.273646</td>\n      <td>148.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-3.615959</td>\n      <td>-9.463472</td>\n      <td>-2.307431</td>\n      <td>0.121294</td>\n      <td>-0.000020</td>\n      <td>1.070539</td>\n      <td>1.066903</td>\n      <td>0.003423</td>\n      <td>10.043770</td>\n      <td>0.015301</td>\n      <td>-0.000230</td>\n      <td>-9.494888</td>\n      <td>-15.940693</td>\n      <td>498.750000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>-3.233402</td>\n      <td>-8.710854</td>\n      <td>-1.537733</td>\n      <td>0.207657</td>\n      <td>-0.000006</td>\n      <td>1.076238</td>\n      <td>1.071413</td>\n      <td>0.004417</td>\n      <td>10.130237</td>\n      <td>0.016210</td>\n      <td>-0.000075</td>\n      <td>-8.177757</td>\n      <td>-14.728990</td>\n      <td>736.500000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>-2.950006</td>\n      <td>-8.218369</td>\n      <td>-1.134104</td>\n      <td>0.247337</td>\n      <td>0.000005</td>\n      <td>1.082333</td>\n      <td>1.077769</td>\n      <td>0.005953</td>\n      <td>10.329087</td>\n      <td>0.016868</td>\n      <td>-0.000003</td>\n      <td>-6.204012</td>\n      <td>-12.952985</td>\n      <td>946.500000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>-1.984958</td>\n      <td>-6.296653</td>\n      <td>0.648827</td>\n      <td>1.474969</td>\n      <td>0.000035</td>\n      <td>1.101465</td>\n      <td>1.094639</td>\n      <td>1.817914</td>\n      <td>13.409150</td>\n      <td>0.020022</td>\n      <td>0.000438</td>\n      <td>-4.260407</td>\n      <td>-9.401950</td>\n      <td>2237.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "numBat1 = len([i for i in list(df.cell_key) if i[1] == \"1\"])\n",
    "numBat2 = len([i for i in list(df.cell_key) if i[1] == \"2\"])\n",
    "numBat3 = len([i for i in list(df.cell_key) if i[1] == \"3\"])\n",
    "numBat = sum((numBat1,numBat2,numBat3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ind = np.hstack((np.arange(0,(numBat1+numBat2),2),83))\n",
    "train_ind = np.arange(1,(numBat1+numBat2-1),2)\n",
    "secondary_test_ind = np.arange(numBat-numBat3,numBat);\n",
    "\n",
    "splits = [train_ind, test_ind, secondary_test_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature and target columns for regression models\n",
    "\n",
    "varmod_features = [\"variance_dQ_100_10\"]\n",
    "dismod_features = [\n",
    "    \"variance_dQ_100_10\",\n",
    "    \"minimum_dQ_100_10\",\n",
    "    \"skewness_dQ_100_10\",\n",
    "    \"kurtosis_dQ_100_10\",\n",
    "    \"discharge_capacity_2\",\n",
    "    \"diff_discharge_capacity_max_2\",\n",
    "]\n",
    "fullmod_features = [\n",
    "    \"minimum_dQ_100_10\",\n",
    "    \"variance_dQ_100_10\",\n",
    "    \"slope_lin_fit_2_100\",\n",
    "    \"intercept_lin_fit_2_100\",\n",
    "    \"discharge_capacity_2\",\n",
    "    \"mean_charge_time_2_6\",\n",
    "    \"minimum_IR_2_100\",\n",
    "    \"diff_IR_100_2\",\n",
    "]\n",
    "targetmod = [\"cycle_life\"]\n",
    "\n",
    "# Define feature and target columns for classifiers\n",
    "\n",
    "varclf_features = [\"variance_dQ_5_4\"]\n",
    "fullclf_features = [\n",
    "    \"minimum_dQ_5_4\",\n",
    "    \"variance_dQ_5_4\",\n",
    "    \"discharge_capacity_2\",\n",
    "    \"diff_IR_100_2\",\n",
    "]\n",
    "targetclf = [\"cycle_550_clf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split(data, features, target, split):\n",
    "    X = data.iloc[split,:].loc[:,features]\n",
    "    y = data.iloc[split,:].loc[:,target]\n",
    "    return X, y\n",
    "\n",
    "def eval_model(model, data, features, target, split):\n",
    "    rmse = list()\n",
    "    mpe = list()\n",
    "    for split in splits:\n",
    "        X, y = get_split(data, features, target, split)\n",
    "        pred = model.predict(X)\n",
    "        rmse.append(np.sqrt(mean_squared_error(pred, y)))\n",
    "        mpe.append(float(np.mean(np.abs((y - pred.reshape(-1,1))) / y * 100)))\n",
    "    return rmse, mpe\n",
    "\n",
    "def eval_classifier(model, data, features, target, splits):\n",
    "    acc = list()    \n",
    "    for split in splits:\n",
    "        X, y = get_split(data, features, target, split)\n",
    "        pred = model.predict(X)\n",
    "        acc.append(accuracy_score(pred, y.values.ravel()))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic Net: 0.778771357057023\n",
      "Linear Regression: 0.779697297534494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Train Elastic net\n",
    "x_train, y_train = get_split(df, varmod_features, targetmod, train_ind)\n",
    "\n",
    "alphas = np.linspace(0.0001,1,30)\n",
    "parameters = {\n",
    "    \"alpha\": alphas,\n",
    "    \"l1_ratio\": [0.01, 0.25, 0.5, 0.75, 1.]\n",
    "}\n",
    "enet = ElasticNet(random_state=54)\n",
    "regr = GridSearchCV(enet, parameters, cv=4)\n",
    "print(\"Elastic Net: %s\" % regr.fit(x_train, y_train).score(x_train, y_train))\n",
    "\n",
    "\"\"\"\n",
    "Because an elastic net with alpha = 0 is technically a linear regression\n",
    "and elastic net produces inaccuracies with a small alpha,\n",
    "we also train a linear regression model.\n",
    "Linear regression performs slighty better at RMSE,\n",
    "Elastic net performs slightly better at MPE.\n",
    "We decide to take the linear regression scores.\n",
    "\"\"\"\n",
    "lin_reg = LinearRegression()\n",
    "print(\"Linear Regression: %s\" % lin_reg.fit(x_train, y_train).score(x_train, y_train))\n",
    "\n",
    "varmod_rmse, varmod_mpe = eval_model(lin_reg, df, varmod_features, targetmod, splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discharge Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic Net: 0.8466638373843819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Train Elastic net\n",
    "x_train, y_train = get_split(df, dismod_features, targetmod, train_ind)\n",
    "\n",
    "alphas = np.linspace(0.1,1,20)\n",
    "parameters = {\n",
    "    \"alpha\": alphas,\n",
    "    \"l1_ratio\": [0.01, 0.25, 0.5, 0.75, 1.]\n",
    "}\n",
    "enet = ElasticNet(random_state=54)\n",
    "regr = GridSearchCV(enet, parameters, cv=4)\n",
    "print(\"Elastic Net: %s\" % regr.fit(x_train, y_train).score(x_train, y_train))\n",
    "\n",
    "dismod_rmse, dismod_mpe = eval_model(regr, df, dismod_features, targetmod, splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.938e+05, tolerance: 4.002e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.994e+05, tolerance: 3.769e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.591e+05, tolerance: 3.756e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.245e+04, tolerance: 4.002e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.311e+05, tolerance: 3.769e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.699e+05, tolerance: 3.756e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.031e+02, tolerance: 6.016e+01\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.075e+04, tolerance: 4.002e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.390e+05, tolerance: 3.769e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.267e+05, tolerance: 3.756e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.438e+03, tolerance: 4.002e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.525e+04, tolerance: 3.769e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.973e+03, tolerance: 3.756e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.729e+02, tolerance: 4.002e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.908e+04, tolerance: 3.769e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.260e+03, tolerance: 3.756e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.230e+04, tolerance: 3.769e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.010e+03, tolerance: 3.769e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.919e+03, tolerance: 3.769e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.645e+03, tolerance: 3.769e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.016e+03, tolerance: 3.769e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.496e+03, tolerance: 3.769e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.350e+03, tolerance: 3.769e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.100e+02, tolerance: 3.769e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.022e+02, tolerance: 3.769e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic Net: 0.9207206035327872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.208e+05, tolerance: 4.281e+02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n",
      "C:\\Users\\PC-1\\Documents\\GitHub\\Projects\\battery_island\\venv39\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Train Elastic net model\n",
    "# raising the alpha minimum to 0.59 silences the convergence warnings,\n",
    "# but decreases the score significantly - what's wrong here? \n",
    "\n",
    "x_train, y_train = get_split(df, fullmod_features, targetmod, train_ind)\n",
    "\n",
    "alphas = np.linspace(0.001,1,20)\n",
    "parameters = {\n",
    "    \"alpha\": alphas,\n",
    "    \"l1_ratio\": [0.001, 0.75, 1.]\n",
    "}\n",
    "enet = ElasticNet(random_state=54)\n",
    "regr = GridSearchCV(enet, parameters, cv=4)\n",
    "print(\"Elastic Net: %s\" % regr.fit(x_train, y_train).score(x_train, y_train))\n",
    "\n",
    "fullmod_rmse, fullmod_mpe = eval_model(regr, df, fullmod_features, targetmod, splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate all linear regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "             Model  RMSE - Train  RMSE - Primary test  RMSE - Secondary test  \\\n0   Variance model    151.665330           166.800507             183.023307   \n1  Discharge model    126.531512           211.116980             225.425486   \n2       Full model     90.982228           131.272456             265.357683   \n\n   MPE - Train  MPE - Primary test  MPE - Secondary test  \n0    21.615414           22.232938             12.874134  \n1    18.271368           22.573476             12.387553  \n2    11.536047           19.161455             20.083662  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>RMSE - Train</th>\n      <th>RMSE - Primary test</th>\n      <th>RMSE - Secondary test</th>\n      <th>MPE - Train</th>\n      <th>MPE - Primary test</th>\n      <th>MPE - Secondary test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Variance model</td>\n      <td>151.665330</td>\n      <td>166.800507</td>\n      <td>183.023307</td>\n      <td>21.615414</td>\n      <td>22.232938</td>\n      <td>12.874134</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Discharge model</td>\n      <td>126.531512</td>\n      <td>211.116980</td>\n      <td>225.425486</td>\n      <td>18.271368</td>\n      <td>22.573476</td>\n      <td>12.387553</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Full model</td>\n      <td>90.982228</td>\n      <td>131.272456</td>\n      <td>265.357683</td>\n      <td>11.536047</td>\n      <td>19.161455</td>\n      <td>20.083662</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\"Model\":[\"Variance model\", \"Discharge model\", \"Full model\"],\n",
    "              \"RMSE - Train\": [varmod_rmse[0],dismod_rmse[0],fullmod_rmse[0]],\n",
    "              \"RMSE - Primary test\": [varmod_rmse[1],dismod_rmse[1],fullmod_rmse[1]],\n",
    "              \"RMSE - Secondary test\": [varmod_rmse[2],dismod_rmse[2],fullmod_rmse[2]],\n",
    "              \"MPE - Train\": [varmod_mpe[0],dismod_mpe[0],fullmod_mpe[0]],\n",
    "              \"MPE - Primary test\": [varmod_mpe[1],dismod_mpe[1],fullmod_mpe[1]],\n",
    "              \"MPE - Secondary test\": [varmod_mpe[2],dismod_mpe[2],fullmod_mpe[2]]})                                                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logreg: 0.8048780487804879\n"
     ]
    }
   ],
   "source": [
    "# Train Logistic Regression\n",
    "x_train, y_train = get_split(df, varclf_features, targetclf, train_ind)\n",
    "\n",
    "parameters = {\"C\": [0.01,0.1,0.5,0.75,1]}\n",
    "\n",
    "logreg = LogisticRegression(solver=\"liblinear\", random_state=54)\n",
    "clf = GridSearchCV(logreg, parameters, cv=4)\n",
    "print(\"Logreg: %s\" % clf.fit(x_train, y_train.values.ravel()).score(x_train, y_train.values.ravel()))\n",
    "\n",
    "varclf_acc = eval_classifier(clf, df, varclf_features, targetclf, splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logreg: 0.6585365853658537\n"
     ]
    }
   ],
   "source": [
    "# Train Logistic Regression\n",
    "# Why is the full classifier worse than the variance classifier?\n",
    "x_train, y_train = get_split(df, fullclf_features, targetclf, train_ind)\n",
    "\n",
    "parameters = {\"C\": [0.01,0.1,0.5,0.75,1]}\n",
    "\n",
    "logreg = LogisticRegression(solver=\"liblinear\", random_state=54)\n",
    "clf = GridSearchCV(logreg, parameters, cv=4)\n",
    "print(\"Logreg: %s\" % clf.fit(x_train, y_train.values.ravel()).score(x_train, y_train.values.ravel()))\n",
    "\n",
    "fullclf_acc = eval_classifier(clf, df, fullclf_features, targetclf, splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate all classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "            Classifier  Acc - Train  Acc - Primary test  Acc - Secondary test\n0  Variance classifier     0.804878            0.767442                  0.95\n1      Full classifier     0.658537            0.627907                  0.60",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Classifier</th>\n      <th>Acc - Train</th>\n      <th>Acc - Primary test</th>\n      <th>Acc - Secondary test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Variance classifier</td>\n      <td>0.804878</td>\n      <td>0.767442</td>\n      <td>0.95</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Full classifier</td>\n      <td>0.658537</td>\n      <td>0.627907</td>\n      <td>0.60</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\"Classifier\":[\"Variance classifier\", \"Full classifier\"],\n",
    "              \"Acc - Train\": [varclf_acc[0],fullclf_acc[0]],\n",
    "              \"Acc - Primary test\": [varclf_acc[1],fullclf_acc[1]],\n",
    "              \"Acc - Secondary test\": [varclf_acc[2],fullclf_acc[2]]})                                  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}